# Copyright (c) authors. All Rights Reserved.

import datetime
from pathlib import Path
import random
import time
import numpy as np
import os
from copy import deepcopy
import math
from functools import partial
import warnings

from torch import nn, optim
import torch
import torch.utils.data as data
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torchvision.transforms import InterpolationMode, Lambda, autoaugment
torchvision.disable_beta_transforms_warning()

from fsc_modeling import create_tasks, PlainTaskDataset, accuracy
from config_args import args
from fvcore.nn import FlopCountAnalysis

timeout_second = 60 * 30
file_store = '/tmp/filestore'


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def drop_path(x, drop_prob: float = 0., training: bool = False):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def get_qkv(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        return qkv[0], qkv[1], qkv[2]

    def forward(self, x, prefix=None):
        B, N, C = x.shape
        q, k, v = self.get_qkv(x)

        if prefix is not None:
            pre_k, pre_v = prefix.chunk(2, dim=0)
            pre_k = pre_k.expand(B, -1, -1, -1)
            pre_v = pre_v.expand(B, -1, -1, -1)
            # try:
            #     k = torch.cat([pre_k, k], 2)
            #     v = torch.cat([pre_v, v], 2)
            # except RuntimeError:
            #     import pdb;pdb.set_trace()
            k = torch.cat([pre_k, k], dim=2)
            v = torch.cat([pre_v, v], dim=2)

        attn_ = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn_.softmax(dim=-1)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)

        # attn_perhead = []
        # for h in range(self.num_heads):
        #     _q = q[:, h:(h + 1)]
        #     _k = k[:, h:(h + 1)]
        #     _v = v[:, h:(h + 1)]
        #     _attn = (_q @ _k.mT) * self.scale
        #     _attn = F.softmax(_attn, dim=-1)
        #     _attn = self.attn_drop(_attn)
        #     attn_perhead.append((_attn @ _v).transpose(1, 2))
        #     del _q, _k, _v, _attn
        #     torch.cuda.empty_cache()
        # x = torch.cat(attn_perhead, dim=2).view(B, N, C)
        # attn_ = None

        x = self.proj(x)
        x = self.proj_drop(x)
        return x, attn_


class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x, prefix=None, return_attention=False):
        y = self.norm1(x)
        y, attn = self.attn(y, prefix=prefix)
        if return_attention:
            return attn
        x = x + self.drop_path(y)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


class Adapter(nn.Module):
    def __init__(self, embed_dim, hidden_dim):
        super().__init__()
        self.offset = nn.Parameter(torch.zeros(embed_dim), requires_grad=True)

    def forward(self, x):
        return self.offset


class Block_Ada(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, adapter_hidden_dim=32):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        self.adapter_1 = Adapter(dim, adapter_hidden_dim)
        self.adapter_2 = Adapter(dim, adapter_hidden_dim)

    def forward(self, x, prefix=None, return_attention=False, use_adapter=True):
        y, attn = self.attn(self.norm1(x), prefix=prefix)
        if return_attention:
            return attn
        tmp = self.drop_path(y)
        if use_adapter:
            tmp = tmp + self.adapter_1(tmp)
        x = x + tmp

        tmp = self.drop_path(self.mlp(self.norm2(x)))
        if use_adapter:
            tmp = tmp + self.adapter_2(tmp)
        x = x + tmp
        return x


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        num_patches = (img_size // patch_size) * (img_size // patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


class VisionTransformer(nn.Module):
    """ Vision Transformer """
    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,
                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,
                 drop_path_rate=0., norm_layer=nn.LayerNorm, global_pool=False, adapter_hidden_dim=-1,
                 **kwargs):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim
        self.global_pool = global_pool
        self.depth = depth

        self.patch_embed = PatchEmbed(
            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule

        if adapter_hidden_dim == -1:
            self.blocks = nn.ModuleList([
                Block(
                    dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
                for i in range(depth)])
        else:
            self.blocks = nn.ModuleList([
                Block_Ada(
                    dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                    drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, adapter_hidden_dim=adapter_hidden_dim)
                for i in range(depth)])

        self.norm = norm_layer(embed_dim)

        # Classifier head
        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        trunc_normal_(self.pos_embed, std=.02)
        trunc_normal_(self.cls_token, std=.02)
        self.apply(self._init_weights)

        if self.global_pool:
            self.fc_norm = norm_layer(self.embed_dim)
            del self.norm  # remove the original norm

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    # def reset_prefix(self,):
    #     if hasattr(self, 'prefix'):
    #         del self.prefix
    #
    # def add_prefix(self, num_prefix, init_value=None):
    #     self.reset_prefix()
    #     if init_value is not None:
    #         weight_k = init_value
    #     else:
    #         weight_k = torch.zeros(self.depth-1, num_prefix, self.embed_dim)
    #     weight_v = torch.zeros(self.depth-1, num_prefix, self.embed_dim)
    #     weight = nn.Parameter(torch.stack([weight_k, weight_v], 1), requires_grad=True)
    #     self.register_parameter('prefix', weight)

    def interpolate_pos_encoding(self, x, w, h):
        npatch = x.shape[1] - 1
        N = self.pos_embed.shape[1] - 1
        if npatch == N and w == h:
            return self.pos_embed
        class_pos_embed = self.pos_embed[:, 0]
        patch_pos_embed = self.pos_embed[:, 1:]
        dim = x.shape[-1]
        w0 = w // self.patch_embed.patch_size
        h0 = h // self.patch_embed.patch_size
        # we add a small number to avoid floating point error in the interpolation
        # see discussion at https://github.com/facebookresearch/dino/issues/8
        w0, h0 = w0 + 0.1, h0 + 0.1
        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),
            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),
            mode='bicubic',
        )
        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]
        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)

    def prepare_tokens(self, x):
        B, nc, w, h = x.shape
        x = self.patch_embed(x)  # patch linear embedding

        # add the [CLS] token to the embed patch tokens
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)

        # add positional encoding to each token
        x = x + self.interpolate_pos_encoding(x, w, h)

        return self.pos_drop(x)

    def forward(self, x, prefix=None, return_feat=False, use_adapter=True):
        x = self.prepare_tokens(x)
        for i, blk in enumerate(self.blocks):
            if prefix is not None:
                x = blk(x, prefix=prefix[0, i], use_adapter=use_adapter)
            else:
                x = blk(x)

        if self.global_pool:
            feat = x[:, 1:].mean(1)
            feat = self.fc_norm(feat)
        else:
            x = self.norm(x)
            feat = x[:, 0]
        if return_feat: return feat
        return self.head(feat)

    def embed(self, x):
        return self.forward(x, return_feat=True)

    def get_last_selfattention(self, x, use_adapter=True):
        x = self.prepare_tokens(x)
        for i, blk in enumerate(self.blocks):
            if i < len(self.blocks) - 1:
                x = blk(x, use_adapter=use_adapter)
            else:
                # return attention of the last block
                return blk(x, use_adapter=use_adapter, return_attention=True)

    def get_intermediate_layers(self, x, n=1):
        x = self.prepare_tokens(x)
        # we return the output tokens from the `n` last blocks
        output = []
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if len(self.blocks) - i <= n:
                output.append(self.norm(x))
        return output

    def get_all_cls_token(self, x):
        x = self.prepare_tokens(x)
        # we return the output tokens from the `n` last blocks
        output = []
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            output.append(self.norm(x)[:, 0])
        return output

    def get_patch_embed(self, x):
        x = self.prepare_tokens(x)
        patch_embed = x.clone()[:, 1:]
        for i, blk in enumerate(self.blocks):
            if i < len(self.blocks) - 1:
                x = blk(x)
            else:
                # return attention of the last block
                attn = blk(x, return_attention=True)

        attn = F.softmax(attn[:, :, 0, 1:], dim=-1).unsqueeze(-1)  # (batch_size, num_head, num_patch, 1)
        patch_embed = (patch_embed.unsqueeze(1) * attn).sum(2).mean(1)
        return patch_embed


def vit_tiny(patch_size=8, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_tiny_adapter(patch_size=8, adapter_hidden_dim=32, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), adapter_hidden_dim=adapter_hidden_dim, **kwargs)
    return model


def vit_small(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_small_adapter(patch_size=16, adapter_hidden_dim=32, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), adapter_hidden_dim=adapter_hidden_dim, **kwargs)
    return model


def vit_base(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def vit_base_adapter(patch_size=16, adapter_hidden_dim=32, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,
        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), adapter_hidden_dim=adapter_hidden_dim, **kwargs)
    return model


class DINOHead(nn.Module):
    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.mlp = nn.Linear(in_dim, bottleneck_dim)
        else:
            layers = [nn.Linear(in_dim, hidden_dim)]
            if use_bn:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.GELU())
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_bn:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(nn.GELU())
            layers.append(nn.Linear(hidden_dim, bottleneck_dim))
            self.mlp = nn.Sequential(*layers)
        self.apply(self._init_weights)
        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))
        self.last_layer.weight_g.data.fill_(1)
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.mlp(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x


class PR(nn.Module):
    def __init__(self, init_prefix, in_dim=384, out_dim=64, warmup_teacher_temp=0.04, teacher_temp=0.04,
                 warmup_teacher_temp_epochs=5, nepochs=40, student_temp=0.1,
                 center_momentum=0.9):
        super().__init__()
        self.student_temp = student_temp
        self.center_momentum = center_momentum
        self.register_buffer("center", torch.zeros(1, out_dim))
        # we apply a warm up for the teacher temperature because
        # a too high temperature makes the training instable at the beginning
        self.teacher_temp_schedule = np.concatenate((
            np.linspace(warmup_teacher_temp,
                        teacher_temp, warmup_teacher_temp_epochs),
            np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp
        ))

        self.projector = nn.Linear(in_dim, out_dim)
        self.init_prefix = init_prefix

    def forward(self, prefix, epoch):
        """
        Cross-entropy between softmax outputs of the teacher and student networks.
        """
        prefix = F.normalize(prefix, dim=-1, p=2)
        init_prefix = F.normalize(self.init_prefix, dim=-1, p=2)

        prefix = self.projector(prefix)
        init_prefix = self.projector(init_prefix)

        temp = self.teacher_temp_schedule[epoch]
        prefix = prefix / self.student_temp
        init_out = F.softmax((init_prefix - self.center) / temp, dim=-1).detach()
        # init_out = F.softmax(init_prefix, dim=-1).detach()

        loss = torch.sum(-init_out * F.log_softmax(prefix, dim=-1), dim=-1).mean()

        self.update_center(init_out)
        return loss

    @torch.no_grad()
    def update_center(self, teacher_output):
        """
        Update center used for teacher output.
        """
        batch_center = torch.mean(teacher_output, dim=0, keepdim=True)

        # ema update
        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)


def apply_selection(features, vartheta):
    """
    Performs pre-classifier alignment of features (feature adaptation) via a linear transformation.
    """

    features = features
    features = F.linear(features, vartheta[0])

    return features


def ft_infer(single_task, model, finetune_loss, gpu_id, args, debug=False):
    infer_model = deepcopy(model)
    infer_model.requires_grad_(False)
    infer_model.eval()

    eval_qset, val_sset, eval_target = single_task
    input_dim = args.channel_size
    lr = args.ett_finetune_lr
    output_dim = input_dim
    n_layer = 12
    n_head = input_dim // 64
    lam = 0.1
    n_hidden = input_dim // 2
    n_way = len(val_sset)

    vartheta = []
    vartheta.append(torch.eye(output_dim, input_dim).cuda(gpu_id))
    control_trans = nn.Sequential(
        nn.Linear(input_dim, n_hidden),  # 1024 * 512
        nn.Tanh(),
        nn.Linear(n_hidden, n_layer * 2 * input_dim)
    ).cuda(gpu_id)

    # start timing
    adapt_start = time.time()
    val_sset = [s.cuda(gpu_id, non_blocking=True) for s in val_sset]
    with torch.no_grad():
        prototype = [infer_model.get_patch_embed(spt).mean(dim=0) for spt in val_sset]  # [(C)]
        init_prefix = torch.stack(prototype, dim=0)  # (L,C)

    if init_prefix is not None:
        prefix_weight = init_prefix.clone().cuda(gpu_id)
    else:
        prefix_weight = torch.randn(n_way, output_dim).cuda(gpu_id)
    for name, param in infer_model.named_parameters():
        if 'adapter' in name:
            param.requires_grad = True
    control_trans.requires_grad_(True)
    prefix_weight.requires_grad_(True)
    vartheta = [v.requires_grad_(True) for v in vartheta]
    optim_list = [{'params': vartheta[0], 'lr': lr},
                  {'params': infer_model.parameters(), 'lr': lr / 2},
                  {'params': control_trans.parameters(), 'lr': lr / 2},
                  {'params': prefix_weight, 'lr': lr / 2}]

    if init_prefix is not None:
        loss_fn = PR(init_prefix, in_dim=input_dim, nepochs=args.finetune_steps).cuda(gpu_id)
    else:
        loss_fn = None
    if loss_fn is not None:
        loss_fn.requires_grad_(True)
        optim_list.append({'params': loss_fn.parameters(), 'lr': lr / 2})
    ft_optimizer = optim.AdamW(optim_list, eps=1e-4)
    # if args.rank == 0 and debug:
    #     trainable = sum(p.numel() for p in infer_model.parameters() if p.requires_grad)
    #     trainable += sum(p.numel() for p in control_trans.parameters() if p.requires_grad)
    #     if loss_fn is not None:
    #         trainable += sum(p.numel() for p in loss_fn.parameters() if p.requires_grad)
    #     trainable += prefix_weight.numel() + vartheta[0].numel()
    #     print('Finetune # train params :', trainable, flush=True)
    #     print('Finetune # variables :', len(optim_list), flush=True)

    # finetune
    ft_steps = args.finetune_steps
    with torch.autograd.set_detect_anomaly(True):
        ft_loss = []
        pseudo_classes = support_classes(val_sset)
        for i in range(ft_steps):
            ft_optimizer.zero_grad()

            prefix = control_trans(prefix_weight).view(n_way, n_layer, 2, n_head, input_dim // n_head)
            prefix = prefix.permute(1, 2, 3, 0, 4)
            prefix = prefix.unsqueeze(0).expand(2, -1, -1, -1, -1, -1)
            support_embeddings = []
            for support_img in val_sset:
                context_features = infer_model(support_img, prefix=prefix, return_feat=True)
                selected_features = apply_selection(context_features, vartheta)
                support_embeddings.append(selected_features)

            prots = torch.stack([se.mean(dim=0) for se in support_embeddings], dim=0).unsqueeze(0)  # (1,L,C)
            embeds = torch.cat(support_embeddings, dim=0).unsqueeze(1)  # (Q,1,C)

            ft_logits = F.cosine_similarity(embeds, prots, dim=-1, eps=1e-30)
            ftloss = finetune_loss(ft_logits / args.temperature, pseudo_classes)  # (K,L), (K)
            if init_prefix is not None:
                dt_loss = loss_fn(prefix_weight, i)
                ftloss = ftloss + lam * dt_loss
            ftloss.backward()
            ft_optimizer.step()

            ft_loss.append(ftloss.item())
    time_task = time.time() - adapt_start
    if debug:
        print(f'Finetune {ft_steps} steps completed in {time_task}s per task', flush=True)
        print('Finetune loss =', ft_loss, flush=True)

    # inference
    eval_qset = eval_qset.cuda(gpu_id, non_blocking=True)
    eval_target = eval_target.cuda(gpu_id, non_blocking=True)
    eval_sset = val_sset
    infer_model.requires_grad_(False)
    infer_model.eval()
    control_trans.requires_grad_(False)
    prefix_weight.requires_grad_(False)
    vartheta = [v.requires_grad_(False) for v in vartheta]
    if loss_fn is not None:
        loss_fn.requires_grad_(False)

    with torch.no_grad():
        prefix = control_trans(prefix_weight).view(n_way, n_layer, 2, n_head, input_dim // n_head)
        prefix = prefix.permute(1, 2, 3, 0, 4)
        prefix = prefix.unsqueeze(0).expand(2, -1, -1, -1, -1, -1)

        target_features = infer_model(eval_qset, prefix=prefix, return_feat=True)
        _q = apply_selection(target_features, vartheta).unsqueeze(1)
        selected_proto = []
        for support_img in eval_sset:
            context_features = infer_model(support_img, prefix=prefix, return_feat=True)
            selected_context = apply_selection(context_features, vartheta)
            selected_proto.append(selected_context)
        _p = torch.stack([sp.mean(dim=0) for sp in selected_proto], dim=0).unsqueeze(0)
        voutput = F.cosine_similarity(_q, _p, dim=-1, eps=1e-30)
        voutput /= args.temperature

    time_latency = time.time() - adapt_start
    acc_time = accuracy(voutput, eval_target)
    acc_time.append(time_task)
    acc_time.append(time_latency)
    del infer_model, control_trans, prefix_weight, vartheta, eval_qset, eval_sset, val_sset, loss_fn
    del prefix, support_embeddings, prots, embeds, ft_logits, ft_loss, ftloss
    del selected_proto, _p, _q, init_prefix, target_features, context_features, selected_context
    torch.cuda.empty_cache()
    return acc_time, eval_target.size(0)


def count_flops(single_task, model, gpu_id, args, debug=False):
    infer_model = deepcopy(model)
    infer_model.requires_grad_(False)
    infer_model.eval()
    assert not args.half_prec  # half precision is poblematic, force to False

    eval_qset, val_sset, eval_target = single_task
    val_sset = [s.cuda(gpu_id, non_blocking=True) for s in val_sset]
    with torch.no_grad():
        prototype = [infer_model.get_patch_embed(spt).mean(dim=0) for spt in val_sset]  # [(C)]
        init_prefix = torch.stack(prototype, dim=0)  # (L,C)

    input_dim = args.channel_size
    lr = args.finetune_lr
    output_dim = input_dim
    n_layer = 12
    n_head = input_dim // 64
    lam = 0.1
    n_hidden = input_dim // 2
    n_way = len(val_sset)

    vartheta = []
    vartheta.append(torch.eye(output_dim, input_dim).cuda(gpu_id))
    if init_prefix is not None:
        prefix_weight = init_prefix.clone().cuda(gpu_id)
    else:
        prefix_weight = torch.randn(n_way, output_dim).cuda(gpu_id)
    control_trans = nn.Sequential(
        nn.Linear(input_dim, n_hidden),  # 1024 * 512
        nn.Tanh(),
        nn.Linear(n_hidden, n_layer * 2 * input_dim)
    ).cuda(gpu_id)
    if init_prefix is not None:
        loss_fn = PR(init_prefix, in_dim=input_dim, nepochs=args.finetune_steps).cuda(gpu_id)
    else:
        loss_fn = None

    for name, param in infer_model.named_parameters():
        if 'adapter' in name:
            param.requires_grad = True
    control_trans.requires_grad_(True)
    prefix_weight.requires_grad_(True)
    vartheta = [v.requires_grad_(True) for v in vartheta]
    optim_list = [{'params': vartheta[0], 'lr': lr},
                  {'params': infer_model.parameters(), 'lr': lr / 2},
                  {'params': control_trans.parameters(), 'lr': lr / 2},
                  {'params': prefix_weight, 'lr': lr / 2}]
    if loss_fn is not None:
        loss_fn.requires_grad_(True)
        optim_list.append({'params': loss_fn.parameters(), 'lr': lr / 2})

    if args.rank == 0 and debug:
        trainable = sum(p.numel() for p in infer_model.parameters() if p.requires_grad)
        trainable += sum(p.numel() for p in control_trans.parameters() if p.requires_grad)
        if loss_fn is not None:
            trainable += sum(p.numel() for p in loss_fn.parameters() if p.requires_grad)
        trainable += prefix_weight.numel() + vartheta[0].numel()
        print('Finetune # train params :', trainable, flush=True)
        print('Finetune # params :', len(optim_list), flush=True)

    # calculate GFLPOS
    infer_model.eval()
    control_trans.eval()
    loss_fn.eval()
    sel_linear = nn.Linear(input_dim, output_dim).cuda(gpu_id)
    sel_linear.eval()

    # with torch.no_grad():
    prefl = FlopCountAnalysis(control_trans, prefix_weight)
    prefl.unsupported_ops_warnings(False)
    prefl.uncalled_modules_warnings(False)
    preflp = prefl.total()

    lsfl = FlopCountAnalysis(loss_fn, (prefix_weight, 0))
    lsfl.unsupported_ops_warnings(False)
    lsfl.uncalled_modules_warnings(False)
    lossflp = lsfl.total()

    bbflp = 0
    selflp = 0
    prefix = control_trans(prefix_weight).view(n_way, n_layer, 2, n_head, input_dim // n_head)
    prefix = prefix.permute(1, 2, 3, 0, 4)
    prefix = prefix.unsqueeze(0).expand(2, -1, -1, -1, -1, -1)
    for qsset in val_sset:
        qsfl = FlopCountAnalysis(infer_model, (qsset, prefix))
        qsfl.unsupported_ops_warnings(False)
        qsfl.uncalled_modules_warnings(False)
        bbflp += qsfl.total()

        context_features = infer_model(qsset, prefix=prefix, return_feat=True)
        selfl = FlopCountAnalysis(sel_linear, context_features)
        selfl.unsupported_ops_warnings(False)
        selfl.uncalled_modules_warnings(False)
        selflp += selfl.total()

    total_flops = (bbflp + preflp + lossflp + selflp) * 3 * args.finetune_steps
    gflops = total_flops / (2.0 ** 30)
    # print('Train FLOPS(GB) =', gflops, flush=True)

    return gflops


def support_classes(suplist):
    class_list = []
    sdevice = None
    for i, support in enumerate(suplist):
        if sdevice is None:
            sdevice = support.device
        ssize = support.size(0)
        class_list.extend([i] * ssize)
    return torch.tensor(class_list, device=sdevice)


def main_worker(gpu, args):
    args.rank += gpu
    torch.cuda.set_device(gpu)
    torch.backends.cudnn.benchmark = True

    # replace the following with your own setting of GPU-server
    torch.distributed.init_process_group(
        backend="nccl",
        init_method=args.dist_url,
        world_size=args.world_size,
        rank=args.rank
    )
    # torch.distributed.init_process_group(
    #     backend="nccl",
    #     store=torch.distributed.FileStore(file_store, args.world_size),
    #     world_size=args.world_size,
    #     rank=args.rank,
    #     timeout=datetime.timedelta(seconds=timeout_second)
    # )

    if args.ett_vit_arch.endswith('small'):
        model = vit_small_adapter(patch_size=args.patch_size, global_pool=False)
    else:
        model = vit_base_adapter(patch_size=args.patch_size, global_pool=False)
    ckpt = torch.load(args.pretrained, map_location='cpu')
    if args.ckey is not None and args.ckey in ckpt:
        print(f'Take key {args.ckey} in provided checkpoint dict')
        ckpt = ckpt[args.ckey]

    if args.ett_backbone.endswith('dino'):
        new_dict = {k.replace('module.', ''): v for k, v in ckpt.items()}
        state_dict = {k.replace('backbone.', ''): v for k, v in new_dict.items()}
        msg = model.load_state_dict(state_dict, strict=False)
    else:
        msg = model.load_state_dict(ckpt, strict=False)
    print('Pretrained weights found at {} and loaded with msg: {}'.format(args.pretrained, msg))

    model = model.cuda(gpu)
    if args.rank == 0:
        print(model, flush=True)
    # Data loading
    imagenet_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    # constant_normalize = transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
    val_transform = transforms.Compose(
        [
            transforms.Resize(
                size=(args.resolution, args.resolution),
                interpolation=InterpolationMode.BICUBIC
            ),
            transforms.ToTensor(),
            imagenet_normalize
            # constant_normalize
        ]
    )

    kwargs = dict(
        batch_size=None,
        num_workers=args.num_workers,
        pin_memory=True,
    )
    valdir = Path(args.root)
    sampled_root = Path(args.sampled_path)
    if sampled_root.exists():
        print('Fixed samples exist and will be used', flush=True)
    else:
        sampled_root = None
    for ds in args.valdata[gpu]:
        # jumpto_next_ds = False
        # # only for replication, not viable for cross-domain
        # if ds.startswith(('ilsvrc', 'cu_birds', 'fungi', 'mscoco')):
        #     args.ett_finetune_lr = 5e-4
        # elif ds.startswith('traffic_sign'):
        #     args.ett_finetune_lr = 5e-3
        # elif ds.startswith('omniglot'):
        #     args.ett_finetune_lr = 5e-4

        fixed_tasks = create_tasks(data_root=valdir, sample_json=ds, sampled_root=sampled_root)
        print(f'{ds} eval starts ..', flush=True)
        acc_round = []
        neval_round = []
        val_dataset = PlainTaskDataset(
            fixed_tasks,
            transform=val_transform
        )
        val_loader = torch.utils.data.DataLoader(val_dataset, **kwargs)

        debug_print = True
        tot_adapt_time = 0
        tot_latency_time = 0
        gflops = 0
        ft_criterion = nn.CrossEntropyLoss().cuda(gpu)
        for j, one_task in enumerate(val_loader):
            if args.gflops:
                gfl = count_flops(one_task, model, gpu, args, debug=debug_print)
                gflops += gfl
                if j % args.print_freq == 0:
                    print(f'{ds} {j}: ma_flops={gflops / (j+1)}GB', flush=True)
            else:
                try:
                    acc1, nevals = ft_infer(one_task, model, ft_criterion, gpu, args, debug=debug_print)
                except torch.cuda.OutOfMemoryError as err:
                    print('OOM @ Rank =', args.rank, 'GPU/data =', gpu, ds, err, flush=True)
                    raise err
                    # jumpto_next_ds = True
                    # break

                top1acc = acc1[0].item()
                adtime = acc1[-2].item() if torch.is_tensor(acc1[-2]) else acc1[-2]
                ltime = acc1[-1].item() if torch.is_tensor(acc1[-1]) else acc1[-1]
                tot_adapt_time += adtime
                tot_latency_time += ltime
                acc_round.append(top1acc)
                neval_round.append(nevals)
                if j % args.print_freq == 0:
                    print(f'{ds} {j} (lr={args.finetune_lr}): acc_num={int(top1acc * nevals / 100)}, tot_num={nevals}, '
                          f'acc={top1acc}, ma_acc={np.mean(acc_round)}, '
                          f'ma_adapt-time={tot_adapt_time / len(acc_round)}s, '
                          f'ma_latency-time={tot_latency_time / len(acc_round)}s', flush=True)
            if debug_print:
                debug_print = False
        # if jumpto_next_ds:
        #     continue

        tdn = ds.split('.json')[0]
        if args.gflops:
            print('#' * 90 + '\n' + tdn + ': avg. FLOPS(GB) per task =', gflops / len(val_loader), flush=True)
            print('#' * 90, flush=True)
        else:
            tot_tasks = len(acc_round)
            nepi_tot = np.sum(neval_round)
            mean_acc = np.mean(acc_round)
            ci95_acc = 1.96 * np.std(acc_round) / np.sqrt(len(acc_round))
            if args.result_file is not None:
                result_path = Path('results/')
                if not result_path.exists():
                    result_path.mkdir()
                np.save('results/' + args.result_file + '_' + tdn + '.npy', np.asarray(acc_round))
                output_log = args.result_file + '_summary.log'
                with open('results/' + output_log, 'a') as log:
                    print('\n' + '#'*90 + '\n' + tdn + ':' + args.result_file, file=log)
                    print('tasks =', tot_tasks, '; avg. adaptation time per task =', tot_adapt_time / tot_tasks,
                          file=log)
                    print(f'{ds} Eval completed: mean_acc={mean_acc}, ci95={ci95_acc}', file=log)
                    print('#' * 90, file=log)
            else:
                print('\n' + '#'*90 + '\ntasks =', tot_tasks, '; avg. adaptation/latency time per task = (',
                      tot_adapt_time / tot_tasks, tot_latency_time / tot_tasks, ')', flush=True)
                print(f'{ds} Eval completed: mean_acc={mean_acc}, ci95={ci95_acc}, num_evals={nepi_tot}', flush=True)
                print('#'*90, flush=True)


def main():
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpuid
    args.rank = 0
    args.dist_url = f'tcp://localhost:{random.randrange(49152, 65535)}'
    args.world_size = torch.cuda.device_count()
    args.valdata = [[tds + '_' + args.test_type + '.json' for tds in args.test_data.split(' ')]]

    args.ett_vit_arch = 'deitsmall'  # 'vitbase'  # OOM
    args.channel_size = 768 if args.ett_vit_arch.endswith('base') else 384
    args.patch_size = 16  # 8 is OOM
    args.temperature = 0.1
    args.resolution = 224
    if args.ett_backbone.endswith('dino'):
        args.ckey = None  # no key in ckpt: 'student', 'teacher'
        pth_file = f'dino_{args.ett_vit_arch}{args.patch_size}_pretrain.pth'
        args.pretrained = args.dinovit_path + '/' + pth_file
    else:
        args.ckey = 'model'
        if args.ett_vit_arch.endswith('base'):
            args.pretrained = args.deit_path + '/deit_base_patch16_224-b5f2ef4d.pth'
        else:
            args.pretrained = args.deit_path + '/deit_small_patch16_224-cd65a155.pth'
    args.ett_backbone = args.ett_vit_arch + str(args.patch_size) + args.ett_backbone.split('_')[-1]

    if args.gflops or not args.npresults:
        args.result_file = None
    else:
        args.result_file = 'ett' + str(args.resolution) + args.ett_backbone

    print('GPU =', os.environ['CUDA_VISIBLE_DEVICES'], args, flush=True)
    torch.multiprocessing.spawn(main_worker, args=(args,), nprocs=args.world_size)


if __name__ == "__main__":
    main()
